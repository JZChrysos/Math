\documentclass{amsart}
\input{C:/Users/jzchr/MathDocuments/preamble_general.tex}

\DeclareMathOperator{\Inv}{Inv}
\DeclareMathOperator{\Av}{Av}

\title{Algebra I Notes}
\author{Jalen Chrysos}

\begin{document}

\maketitle
\begin{abstract}
	These are my notes from Victor Ginzburg's Representation Theory (Math 325) class at UChicago, Autumn 2025.
\end{abstract}

\tableofcontents

\newpage

\section{Introduction}

%Representations arise from actions. Suppose a group $G$ acts on a set $X$. Then $G$ also acts linearly on $\Hom(X,\C)$ (the space of maps $X\to\C$) via 
%$$
%g : f \mapsto g\circ f,
%$$
%hence it is in isomorphism with $\GL(\Hom(X,\C))$. \\

In this class we'll be interested in the representations of matrix groups. Something like $\GL(V)$ or $\SO(V)$ clearly acts on $V$, but it can also act on other interesting spaces. One relevant case of this for us will be when $G$ acts on polynomials in $x_1,\dots,x_n$. Let 
$$
P_d \subseteq \C[x_1,\dots,x_n]
$$
be the subspace of homogeneous degree-$d$ polynomials in $n$ variables. This space has a basis given by the monomials
$$
\big\{x_1^{d_1}x_2^{d_2}\cdots x_n^{d_n} \; | \; \sum_i d_i = d\big\}
$$
and hence is finite-dimensional. $P_d$ is stable under action by $\GL_n$. This is because linear transformation does not affect the degree of monomials (every $x_j$ is sent to a linear combination of $x_1,x_2,\dots,x_n$).\\

Consider the case of $G=\OO_n$, the orthogonal group. This group fixes the polynomial 
$$R:=x_1^2+\dots+x_n^2$$
so as a result, multiplication by $R$ is an intertwining map $P_d\to P_{d+2}$, meaning $R\circ g^*f = g^*(R\circ f)$.

Likewise, let
$$
\Delta := \sum_j \frac{\partial^2}{\partial x_j^2}
$$
be the Laplacian. This $\Delta$ is an $\OO_n$-intertwining operator.

We call a function $f$ \textit{harmonic} if it has $\Delta(f)=0$. The space of harmonic polynomials in $n$ variables of degree $d$ is denoted $H_d\subseteq P_d$. For $d\in \{0,1\}$, $H_d=P_d$, but for $d\geq 2$ $H_d$ is strictly smaller. Note that $H_d$ is stable under orthogonal transformations.\\

We will now work toward showing that $H_d$ is an irreducible $\SO_n$-representation for $n\geq 3$.\\

A representation $\rho:G\mapsto \GL(V)$ is \textit{unitary} if $G$ always acts as a unitary operator (i.e. preserves Hermitian inner product) on $V$. We can give an inner product between polynomials (or any functions) by
$$
\<f_1,f_2\> := \int_{\R^n} f_1(x)\overline{f_2(x)} e^{-|x|^2} \; \d x
$$
where $\d x$ is the Lebesgue measure. Action of $\SO_n$ on $P_d$ preserves this inner product.

Alternatively, we could put an inner product on $P_d$ (or on all functions) from integration over $S^{n-1}$ (the sphere). And polynomials in $P_d$ are determined by their behavior on $S^{n-1}$.\\

\noindent \textbf{Proposition}: If $V$ is a finite-dimensional vector space with an inner product, then any \textit{unitary} action of $G$ on $V$ is completely reducible. Specifically, if $W\subseteq V$ is a $G$-stable subspace, then one can decompose the action into $V=W\oplus W^{\bot}$.
\begin{proof}
	The thing that we need to prove is that if $W$ is $G$-stable then $W^{\bot}$ is as well. Let $x\in W^{\bot}$ and $w\in W$. Because $g$ acts as a \textit{unitary} operator, we have
	$$
	\<g\cdot x,w\> = \<x,g^{-1}\cdot w\> = 0
	$$
	since $g^{-1}\cdot w \in W$ by $G$-stability of $W$.
\end{proof}\\

\noindent \textbf{Key Lemma}: If $F\subseteq C(S^{n-1})$ is any subspace stable under $\SO_n$, then it has an element fixed by $\SO_{n-1}$.
\begin{proof}
	Let $N:=(0,0,\dots,0,1)\in S^{n-1}$. We have the evaluation map $\a:C(S^{n-1})\to \C$ given by evaluating functions at $N$. We have an inner product on $F$ given by 
	$$
	\<f,g\> := \int_{S^{n-1}} f\overline{g}
	$$
	which is clearly fixed by $\SO_n$, thus $F$ is a unitary representation of $\SO_n$. By Riesz representation theorem, $\a(f)\equiv \<f,\ph\>$ for some $\ph\in F$. For any $g\in \SO_{n-1}$, $g$ fixes $N$, thus
	$$
	\<f,\ph\> = f(N) = f(gN) = (g^{-1}f)(N) = \<g^{-1}(f),\ph\> = \<f,g\ph\>
	$$ 
	and because this is true for arbitrary $f\in F$ and $g\in SO_{n-1}$, $\ph$ is fixed by $\SO_n$. Now it remains to show that $\ph\neq 0$. We can get this by assuming that some function in $F$ takes a nonzero value on $N$ (we can move $N$ to some point where this is true, since $F$ contains a nonzero function).
\end{proof}\\

We can apply this key lemma to $P_d$ or $H_d$ as $F$.\\

Consider $P_d^{\SO_{n-1}}$, the homogeneous polynomials fixed by $\SO_{n-1}$. On homework we showed that this is a subspace of $\C\<x_n,R\>$ (where $R:=x_1^2+\dots+x_n^2$). One basis of this space is
$$
P_d^{\SO_{n-1}} := \C\<x_n^d,x_n^{d-2}R,x_n^{d-4}R^2,\dots\>
$$
thus $\dim(P_d^{\SO_{n-1}})=\lfloor \tfrac{d}{2} \rfloor + 1$. 

A very important fact about $P_d$ is that it decomposes into the subspaces
\begin{align*}
P_d &= H_d \oplus R\cdot P_{d-2}\\
&= H_d \oplus R H_{d-2} \oplus R^2 H_{d-4} \oplus R^3H_{d-6} \oplus \cdots 
\end{align*}
(we will show this later). This allows us to deduce the dimension of $H_d$ from $P_d$:
$$
\dim(H_d) = \dim(P_d) - \dim(P_{d-2}) = {d+2 \choose 2} - {d \choose 2} = 2d+1.
$$
Likewise, we can decompose $P_d^{\SO_{n-1}}$ the same way:
\begin{align*}
P_d^{\SO_{n-1}} &= H_d^{\SO_{n-1}} \oplus R P_{d-2}^{\SO_{n-1}} \\
&= H_d^{\SO_{n-1}} \oplus R H_{d-2}^{\SO_{n-1}} \oplus R^2 H_{d-4}^{\SO_{n-1}} \oplus R^3H_{d-6}^{\SO_{n-1}} \oplus \cdots 
\end{align*}
%Because we already know the dimension of $P_d^{\SO_{n-1}}$, we can use this decomposition to see that $\dim(H_{d-2j}^{\SO_{n-1}})=1$ for all $j$; the dimension of each of these spaces is at least one by the Key Lemma, since $H_{d-2j}$ is stable under action of $\SO_n$, and this already hits the total of $\lfloor d/2\rfloor + 1$, so the dimension must be exactly one.
which gives us the dimension of $H_d^{\SO_{n-1}}$ as
$$
\dim(H_d^{\SO_{n-1}}) = \dim(P_d^{\SO_{n-1}}) - \dim(P_{d-2}^{\SO_{n-1}}) = (\lfloor d/2\rfloor + 1) - (\lfloor (d-2)/2\rfloor + 1) = 1.
$$
As a consequence, we see that each $H_d$ is an \textit{irreducible} representation of $\SO_n$; every stable subspace has a fixed point by the Key Lemma, and the fixed-points are one-dimensional, so there can only be one stable subspace. Thus, as an $\SO_n$-representation, $P_d$ decomposes exactly into the sequence $H_{d-2j}$ for $2j\leq d$.\\

\noindent \textbf{Theorem}: If $n\geq 3$, then for each $d\geq 0$, the representation of $\SO_n$ in $H_d$ is irreducible, and moreover the representations are all distinct for different $d$.\footnote{In the case $n=3$ this gives \textit{all} the irreps. In general you miss $\Lambda^2(\C^n)$, but when $n=3$ this is just $\C^3$, which you get from $H_1$.}

\begin{proof}
	To show that the representations are distinct, we can use a homework problem which shows that the dimension of $H_d$ is always increasing in $d$ for any $n\geq 3$.
\end{proof}\\

\subsection{Differential Algebra}

Let $W$ be a vector space over $k$ with basis $w_1,\dots,w_n$, and let $x_1,\dots,x_n$ be a dual basis for $W^*$. We let
$$
k[W] := \bigoplus_{d\geq 0} k[W]_d
$$
be the homogeneous polynomials over $W$, where 
$$
k[W]_j := \Sym^j(W^*).
$$
We have the directional derivative operator
$$
\partial_{\xi} : k[W]_j \to k[W]_{j-1}
$$
which acts on $k[W]$ in the expected way (product rule). Thus we have the differential algebra
$$
\D(W) = k[\partial_{w_1},\dots,\partial_{w_n}]
$$
acting on $k[W]$. There is a natural correspondence between $k[W]$ and $\D(W)$, if one assumes that $k$ is characteristic 0. We have a $k$-bilinear pairing 
$$
\D(W) \times k[W] \to k
$$
by $\<u,f\>\mapsto u(f)(0)$. This is a \textit{perfect pairing}. And in general we can do the same thing with 
$$
\Sym^j(W)\times \Sym^j(W^*)\to k.
$$

\noindent \textbf{Lemma}: Let $\xi\in W$ and $f\in k[W]$. Then
$$\<\xi^m,f\> = m!f(\xi).$$
In particular, if $f=\ph\in W^*$, $\<\xi^m,\ph^m\>=m!\ph^m(\xi)$.
\begin{proof}
	We will show this for homogeneous $f$ first, and the general result will follow from expressing $f$ as a sum of homogeneous polynomials. Let the degree of $f$ be $d$. Then by Taylor expansion,
	$$f(\xi) = \sum_{k\geq 0} \frac{1}{k!}(\partial_{\xi}^kf)(0).$$
	But note that only the $d$th term of this is nonzero, since $\partial^j_{\xi}f= 0$ unless $j=d$ (since we are evaluating at $0$). Thus,
	$$
	f(\xi) = \frac{(\partial^d_{\xi}f)(0)}{d!}
	$$
	and for other $j$ both sides are 0.
\end{proof}\\

We can use this pairing to get another inner product on polynomials in $k[W]$ given by
$$
\<p,q\> := p(\partial)(q)(0)
$$
where $p(\partial)$ is the corresponding element to $p$ in $\D(W)$.\footnote{In the homework, we establish that on $H_d$, this is actually \textit{equivalent} to the inner product from integrating over $S^{n-1}$!} For this inner product, we have that multiplication by $p$ is \textit{adjoint} to $p(\partial)$, i.e.
$$
\<r,p(\partial)q\> = \<pr,q\>.
$$
With this fact, we can finally show why $P_d = H_d \oplus R P_{d-2}$:
$$
W = \ker(\Delta) \oplus \im(\Delta^*) = \ker(\Delta) \oplus \im(R) = H_d \oplus R P_{d-2}.
$$

Another application of this pairing:
Let $V$ be a finite dimensional vector space and $A\subseteq V$ a subset of $V$ (not necessarily subspace).
Let $\Span^d(A)\subseteq \Sym^d(V)$ be generated over $\C$ by $a^d$ for $a\in A$.
If $A$ is dense in $V$ then $\Span^d(A) = \Sym^d(A)$. 
We will show this by using the pairing.

Assume for contradiction that $\Span^d(A)\neq \Sym^d(V)$. 
Then there is some nonzero linear functional $F:\Sym^d(V)\to \C$ which vanishes on $\Span^d(A)$. 
Then $F$ corresponds to some differential polynomial $f$, and $\partial^d_a f(0) = 0$ for all $a\in A$.
But $\partial^d_af(0) = d!f(a)$, so $f(a)=0$. But then $A$ is dense, so $f=0$.\\

\subsection{Representation Theory Basics}
 
If $G$ acts on sets $X$ and $Y$, then $G$ can also act on the space of maps $X\to Y$ via conjugation:
$$
g: f \mapsto g\circ f \circ g^{-1}.
$$
We can ask about the space of maps which commute with this $G$-action. Or, equivalently, the maps which are fixed by the $G$-action. 
We call these \textit{intertwining operators}. 
The set of such operators is denoted $\Hom_G(X,Y)$. 

We are usually interested in the case where $X,Y$ are vector spaces and $\Hom(X,Y)$ is the space of linear maps.\\

\noindent \textbf{Schur-Weyl Duality}: 
Let $W$ be a finite-dimensional vector space over $\C$.
$\GL(W)$ can act on $W^{\otimes d}$ with $g$ acting as $g^{\otimes d}$.
$S_d$ also acts on $W^{\otimes d}$ by permutation.
It is not too hard to see that these two actions commute.
But moreover, action by $\GL(W)$ \textit{spans} the space of $S_d$-intertwiners on $W^{\otimes d}$.

\begin{proof}
	Let $\Phi:(\End(W))^{\otimes d}\to \End(W^{\otimes d})$ be given by
	$$
	\Phi: a_1\otimes \cdots \otimes a_d \mapsto \big(w_1\otimes \cdots \otimes w_d \mapsto a_1(w_1)\otimes \cdots \otimes a_d(w_d)\big).
	$$
	$\Phi$ is an invertible linear map with inverse
	$$
	\Phi^{-1} f \mapsto f|_{W_1}\otimes \cdots \otimes f|_{W_d}
	$$ 
	where $W_j$ is $0 \otimes \cdots \otimes W \otimes \cdots \otimes 0$ with the $W$ in the $j$th spot.
	Note also that $\Phi$ commutes with the action of $S_d$. By using $\Phi$, we see that 
	$$
	\Sym^d(\End W) = ((\End W)^{\otimes d})^{S_d} \xrightarrow{\Phi^{-1}} \End_{S_d}(W^{\otimes d}).
	$$
	So we only need to understand $\Sym^d(\End W)$. But $\GL(W)$ is dense in $\End(W)$, so by a previous lemma, we see that $\Span^d(\GL(W))=\Sym^d(\End W)$.
\end{proof}\\

\subsection{Spectral Theorem}

Let $A$ be a $k$-algebra with $a\in A$. We have an evaluation map
$$
\ev_a : k[t] \to A \;\;\; \ev_a: p\mapsto p(a).
$$
Let $A_a:= \im(\ev_a)$, i.e. the subalgebra of $A$ generated by $a$. The kernel $\ker(\ev_a)$ is an ideal of $k[t]$, and it is a principal ideal since $k[t]$ is a PID. Thus, in the case that $\ev_a$ is non-injective, there is a unique \textit{minimal polynomial} of $a$, $p_a$, which divides every polynomial which vanishes at $a$.\\

\noindent \textbf{Lemma}: $a$ is algebraic iff $A_a$ is finite-dimensional.
\begin{proof}
	If $A_a$ is finite-dimensional then there is a relation between $1,a,a^2,\dots,a^n$ for some $n$, i.e. a polynomial that $a$ solves. Conversely if $a$ solves a polynomial of degree $n$ then every linear combination of powers of $a$ can be expressed by the first $n$ powers of $a$.
\end{proof}\\

We define the \textit{spectrum} of $a$, denoted $\Spec(a)$, as
$$
\Spec(a) := \{\lambda \in k : (a-\lambda) \text{ is not invertible}\}.
$$
So for example, if $A$ is a function algebra, $\Spec(a)$ denotes the values that $a$ can take. In the case that $A$ is the matrix algebra $M_n(k)$, $\Spec(a)$ is the set of eigenvalues of $a$.\\

\textbf{The Spectral Theorem}: Let $A$ be a $k$-algebra of one of the following types:
\begin{itemize}
	\item $A$ is finite-dimensional over $k$ and $k$ is algebraically closed.
	\item $A$ is countable-dimension and $k$ is uncountable.
\end{itemize}
Then,
\begin{enumerate}[(i)]
	\item $\Spec(a)$ is nonempty.
	\item $a$ is nilpotent iff $\Spec(a)=\{0\}$.
	\item If $A$ is a division algebra then $A=k$.\footnote{For a counterexample of this when $k$ is not algebraically closed, take the Quaternions over $\R$.}
\end{enumerate}
\begin{proof}
	Lemma: If $\lambda_1,\dots\lambda_n\not\in \Spec(a)$, i.e. $(a-\lambda_j)$ is invertible for each $j$, then if 
	$$
	\sum_j c_j(a-\lambda_j)^{-1} = 0
	$$
	for some $c_j\in k$ then $a$ is algebraic (proof is by clearing denominators). We will use this fact.\\
	
	(i): We will split into two cases: if $a$ is algebraic then $\Spec(a)$ is finite but nonempty and if $a$ is not algebraic then $\Spec(a)$ is uncountable (and the converses to both of these are true).
	
	If $a$ is algebraic, then $\Spec(a)$ is the roots of the minimal polynomial (HW), and particular this means $\Spec(a)$ is finite and nonempty because $k$ is algebraically closed.
	
	If $a$ is not algebraic, then by the Lemma, there is no linear relation between any finitely-many $(a-\lambda)^{-1}$ for $\lambda\not\in \Spec(a)$. We assumed that $\dim(A)$ is at most countable, and it has an independent set of size $|k\setminus \Spec(a)|$, so $\Spec(a)$ must be uncountable (because $k$ is).\\
	
	(ii): If $a^n=0$, then $0\in \Spec(a)$ because $a$ is not invertible, but all other $(a-\lambda)$ are invertible:
	$$
	(a-\lambda)(a^{n-1}+a^{n-2}\lambda + a^{n-3}\lambda^2 + \dots + \lambda^{n-1}) = a^n-\lambda^n = -\lambda^n.
	$$
	so
	$$
	(a-\lambda)^{-1} = -\lambda^{-n}(a^{n-1}+a^{n-2}\lambda + a^{n-3}\lambda^2 + \dots + \lambda^{n-1}).
	$$
	Conversely, suppose that $\Spec(a)=\{0\}$. $\{0\}$ is a finite set, so by part (i), $a$ is algebraic, but its minimal polynomial only has root $a=0$, so $a^n=0$ for some $n$.
	\\
	
	(iii): Assume for contradiction that $A$ is a division algebra yet $\exists a\in A\setminus k$. Then $(a-\lambda)$ is invertible for all $\lambda \in k$, but then $\Spec(a)$ would be empty, contradicting (i).
\end{proof}\\

\subsection{Modules}

A module $M$ over ring $A$ is called \textit{simple} if it is nonzero and has no proper nontrivial submodules (i.e. its only submodules are $0$ and $M$).\\

\noindent \textbf{Schur's Lemma}: If $f:M\to N$ is an $A$-linear map between \textit{simple} $A$-modules $M$ and $N$, then $f$ is either $0$ or an isomorphism.
\begin{proof}
	$\ker(f)$ is a submodule of $M$ and $\im(f)$ is a submodule of $N$. By simplicity, both must be either trivial or the full module. This implies that $f$ is either injective or 0, and either surjective or 0.
\end{proof}\\

As a corollary, we see that $\End_AM$ is a division ring.\\

\noindent \textbf{Schur's Lemma for Algebras}: If $A$ is a $k$-algebra and $M$ a simple $A$-module either 
\begin{itemize}
	\item $k$ is algebraically closed and either $A$ or $M$ is finite-dimensional over $k$.
	\item $k=\C$ and either $A$ or $M$ is countable-dimension over $k$.
\end{itemize}
Then, $\End_AM = k \id_M$.
\begin{proof}
	On HW we showed that $\dim(\End_AM) \leq \dim_A M$. The lemma can be proven by applying the spectral theorem to the algebra $\End_AM$.
\end{proof} \\

If $A$ satisfies the hypotheses of the Spectral Theorem and $M$ is a simple $A$-module, then the center $Z$ of $A$ acts in $M$ by scalars, as $z\cdot am = az\cdot m$ for $z\in Z, a\in A, m\in M$. And in particular if $A$ is commutative then $\dim_k M=1$ because every subspace of $M$ is $A$-stable.\\

\noindent \textbf{Schur's Lemma for Group Representations}: If $V,W$ are representations of a group $G$ over a field $k$,
\begin{enumerate}[(i)]
	\item If $V,W$ are irreducible then all intertwiners are either 0 or isomorphisms.
	\item If $\dim_k(V)$ is finite and $k$ is algebraically closed or $k=\C$ and $|G|=\aleph_0$, then 
	$$
	\End_GV= k\cdot \id_V.
	$$
\end{enumerate}
\begin{proof}
	This follows from applying Schur's Lemma for Algebras. A representation of $G$ corresponds to a module over the group algebra $A:= kG$. Note that $\dim_k(A) = |G|$.
\end{proof}\\

\subsection{Representations of $S_n$} $S_n$ acts on $\R^n$ by permuting the coordinates. We have the sign representation given by taking the determinant.\\

$S_n$ also acts on $P_d$ by permuting the variables:
$$
\sigma(f)(x_1,\dots,x_n) := f(x_{\sigma^{-1}(1)},x_{\sigma^{-1}(2)},\dots,x_{\sigma^{-1}(n)}).
$$
Motivated by this action, we can consider the symmetric polynomials $P^{S_n}$.\\

A \textit{partition} of $n$ is a finite non-increasing sequence of positive integers $\lambda_1\geq \dots \geq \lambda_k$ whose sum is $n$. Let the set of partitions of $n$ be $\PP_n$. Corresponding to a partition, we have a decomposition of $[1,n]$ into $I_1,I_2,\dots,I_k$ of length $|I_j|=\lambda_j$.\\

The \textit{Vandermonde Determinant} is the polynomial
$$
\Delta_n := \prod_{1\leq i < j \leq n} (x_j-x_i)
$$
which can also be written as the determinant 
$$
\det \begin{pmatrix}
	1 & 1 & \dots & 1\\
	x_1 & x_2 & \dots & x_n \\
	x_1^2 & x_2^2 & \dots & x_n^2\\
	\vdots & \vdots & \ddots & \vdots \\
	x_1^{n-1} & x_2^{n-1} & \dots & x_n^{n-1}
\end{pmatrix}
$$
Corresponding to a given partition $\lambda$, define
$$
\Delta(I_m) := \prod_{i<j \in I_m} (x_j-x_i)
$$
and 
$$
\Delta_{\lambda} := \prod_m \Delta(I_m).
$$
$\Delta_{\lambda}$ is a homogeneous polynomial, and its degree is 
$$
d := \sum_{m} \frac{\lambda_m(\lambda_m-1)}{2}
$$
so $\Delta_{\lambda}\in P_d$.\\ 

The \textit{Specht Module} associated with $\lambda$, denoted $V(\lambda)$ is the $k$-span of $\Delta_{\lambda}$ under the action of $S_n$. It is clearly stable under the action of $S_n$.\footnote{This is not the most common way to construct the Specht module of $\lambda$.}\\

Examples:
\begin{itemize}
	\item Let $\lambda = (1,1,1,\dots,1)$. Then $\Delta_{\lambda} = 1$,  and $V(\lambda) = P_0$, the constant polynomials. The action of $S_n$ on $V(\lambda)$ is trivial. Thus, this $\lambda$ represents the trivial representation.
	\item Let $\lambda = (n)$. Then $\Delta_{\lambda} = \Delta_n$, the entire Vandermonde determinant. Since this is just a determinant whose columns are permuted by the action of $S_n$, the action scales by the sign of the permutation. This makes $V(\lambda) = k\Delta_n$, which is one-dimensional. It is the sign representation of $S_n$.
\end{itemize}
Note that in all of these cases $V(\lambda)$ is irreducible. This is actually true in general:\\

\textbf{Theorem}: Assuming that the underlying field $k$ is characteristic 0, the Specht module is always an irreducible representation. Moreover, all irreps of $S_n$ can be expressed as $V(\lambda)$ for some partition $\lambda$.\footnote{It is also true that $V(\lambda)$ is a subspace of the $S_n$-harmonic polynomials (as defined on HW) and the index is $\dim(V(\lambda))$.}

\begin{proof}
	The proof has three steps. The first step will be to show that $V(\lambda)$ is irreducible, which we do on homework. Step 2 is that the number of irreducible representations of $S_n$ is equal to the number of partitions of $n$. Step 3 will show that the modules $V(\lambda)$ are pairwise non-isomorphic for different $\lambda$, and hence we have a bijection.\\
	
	Step 3: In homework (it is fairly clear I think) we showed that if $d_{\mu}\neq d_{\lambda}$ then $V(\mu)\not\cong V(\lambda)$, so it remains to show this for $\mu,\lambda$ that have equal degree.
	
	Notation: for $\nu \in \Z_{\geq 0}^n$ (note: may have repeats!), $S_n$ acts on $\nu$ in the natural way. Let $m_j(\nu)$ denote the number of elements of $\nu$ that are equal to $j$ (this is invariant under $S_n$). Let $\nu(\lambda)$ be 
	$$
	\nu(\lambda) := (1,2,\dots,\lambda_1,1,2,\dots,\lambda_2,\dots ,1,2,\dots,\lambda_n).
	$$
	Then $m_j(\nu(\lambda))$ is the length of the $j$th column in the Young diagram of $\lambda$, $D(\lambda)$. Or equivalently, the $m_j$ form another partition corresponding to the transposed Young diagram $D^T(\lambda)$.
	
	Similarly, we can apply all this to polynomials in $n$ variables. The monomial $x^{\nu}$ is
	$$
	x^{\nu} := \prod_{j=1}^nx_j^{\nu_j}
	$$
	so that for a partition $\lambda$,
	$$
	x^{\nu(\lambda)} := \prod_{i=1}^k \prod_{j=1}^{\lambda_i} x_{\lambda_i + j}^{j}.
	$$
	Recall that $\Delta_{\lambda}$ is defined in terms of Vandermonde determinants, which are expressed as
	$$
	\Delta_n = \sum_{\sigma \in S_n} \sgn(\sigma) x_1^{\sigma(1)-1}\cdots x_n^{\sigma(n)-1}.
	$$
	And for $\Delta_{\lambda}$, we have a similar thing but with Young subgroups:
	$$
	\Delta_{\lambda} = \sum_{\sigma \in S_{\lambda}} \sgn(\sigma) x_1^{\sigma(1)-1} \cdots x_n^{\sigma(n)-1} =  \frac{1}{x_1x_2\cdots x_n} \sum_{\sigma \in S_{\lambda}} \sgn(\sigma) x^{s(\nu(\lambda))}. 
	$$
	Now, if $V(\mu)=V(\lambda)$, then $\Delta_{\mu}\in V(\lambda)$, i.e. it is a linear combination of permutations of $\Delta_{\lambda}$. So for example, the monomial $x^{\nu(\mu)}$ appears as $x^{\sigma(\nu(\lambda))}$ for some $\sigma$. But we can show that this fails (\textit{fill in later}).
\end{proof}\\

The \textit{Young Subgroup} of $S_n$ corresponding to $\lambda$ consists of all permutations which preserve all the pieces $I_m$. It is denoted $S_{\lambda}$ and is isomorphic to $S_{\lambda_1}\times S_{\lambda_2} \times \dots \times S_{\lambda_k}$.

Define $P^{S_{\lambda}}$ to be the polynomials fixed by $S_{\lambda}$, and $P^{\sgn(\lambda)}$ the polynomials on which $S_{\lambda}$ acts in an anti-symmetric way. We can see that $P^{\sgn(\lambda)}$ is stable under scaling by $P^{S_{\lambda}}$, i.e. it is a $P^{S_{\lambda}}$-submodule.\\

\textbf{Lemma}: Let $F:V(\lambda)\to P_d$ be an $S_n$-intertwiner. Then
\begin{enumerate}
	\item If $d=d_{\lambda}$, then $F$ acts by scaling.
	\item If $d<d_{\lambda}$, then $F$ is trivial.
\end{enumerate}
\begin{proof}
	For $s\in S_{\lambda}$, $s(F(\Delta_{\lambda})) = F(s(\Delta_{\lambda}))$, and $s(\Delta_{\lambda}) = \sgn_{\lambda}(s)\cdot \Delta_{\lambda}$, so $F$ acts by scaling. \textit{fill in later}
\end{proof}\\

In general, we can see that $V(\lambda^t) = V(\lambda)\otimes \sgn$.\\

\subsection{Hilbert's Nullstellensatz} 
Let $k$ be an algebraically closed field and let $P=k[x_1,\dots,x_n]$. An \textit{algebraic} subset of $k^n$ is the vanishing set of an ideal $I\subset P$, denoted $V(I)$. In the other direction, we have an ideal $I_V$ corresponding to polynomials vanishing on a given algebraic set $V$.\\

For any ideal $I\subset P$, there is a \textit{radical} of $I$, denoted $\sqrt{I}$, which consists of all elements of $P$ for which some power lies in $I$. Note that $V(I)=V(\sqrt{I})$. Moreover, ideals of the form $I_V$ are already radical. The interesting thing is that the correspondence goes both ways:\\

\textbf{Nullstellensatz}: Algebraic sets are in bijection with \textit{radical} ideals of $P$, via $V \mapsto \sqrt{I_V}$ and $V(I) \mapsfrom I$. This bijection restricts to one between single points of $k^n$ and maximal ideals of $P$.
\begin{proof}
	We will show that $I_{V(I)}=\sqrt{I}$, the content of which is that every polynomial $f$ vanishing on $V(I)$ has $f^n\in I$ for some $n$.\\
	
	Lemma: for $z\in k^n$, let $\ev_z:P\to k$ be the algebra homomorphism given by evaluation at $z$, i.e. $\ev_z: f\mapsto f(z)$. In fact, \textit{every} algebra homomorphism $\chi: P\to k$ is $\ev_z$ for some $z$. In particular, take $z=(\chi(x_1),\chi(x_2),\dots,\chi(x_n))$ and note that $\chi(f) = \ev_z(f).$\\
	
	Now, let $A$ be a commutative $k$-algebra as in the setting of the Spectral theorem (i.e. finite-dimensional or countable dimension with uncountable $k$). We claim that all maximal ideals of $A$ have the form $\ker(\chi)$ for some algebra homomorphism $\chi: A \to k$, and moreover $\Spec(a)$ is exactly $\{\chi(a) : \chi\in \Hom(A,k)\}$.
	
	To prove this, let $I\subset A$ be a maximal ideal. This implies that $A/I$ is a field, and in particular a division algebra, so the spectral theorem implies that $A/I=k$. Thus the projection onto $I$ gives a character $A\to A/I=k$.\\
	
	Now for the general case. Suppose $f$ vanishes on $V(I)$ but none of its powers is in $I$. Let $\overline{f}$ be $f$ mod $I$. The assumption that $f\not\in \sqrt{I}$ is equivalent to saying that $\overline{f}$ is not nilpotent, so by the spectral theorem, $\Spec(\overline{f})\neq \{0\}$. Thus, there is some $\lambda\neq 0$ so that $\overline{f}-\lambda$ is non-invertible in $A$. Now consider the ideal
	$$
	I + P\cdot (f-\lambda) \subset P.
	$$
	Because $(\overline{f}-\lambda)$ is non-invertible, there can be no $b\in P$ such that $b(\overline{f}-\lambda)=1$ mod $I$, thus $I + P\cdot (f-\lambda)$ is not all of $P$. By the maximal case, this shows that 
	$$
	V(I)\cap V(f-\lambda) = V(I+P\cdot (f-\lambda)) \neq \emptyset 
	$$
	but this implies that $f=\lambda$ on at least one point of $V(I)$, which contradicts the assumption that $f$ vanishes on $V(I)$.
\end{proof}\\

As a corollary, we see that every proper ideal $I\subsetneq P$ there is some $z$ such that $f(z)=0$ for $z\in I$. This is because otherwise $V(I)=0$, so $\sqrt{I}=P$.

Another corollary: if $f_1,\dots,f_r\in P$ have no common solution, then there is some $P$-linear combination of these equal to 1, i.e. $1\in (f_1,\dots,f_r)$.

A third corollary: if $V(I_1)\cap V(I_2)=\emptyset$, then there is some polynomial $f$ such that $f=0$ on $V(I_1)$ and $f=1$ on $V(I_2)$. To get such an $f$, we use the fact that $I_1+I_2=P$, so there is $f\in I_1$ and $g\in I_2$ such that $f+g=1$, so we can take this $f$.\\

\subsection{Invariant Theory} Let $G$ act on $X$. If $f$ is a $G$-invariant function on $X$, another way to think of this is that $f$ takes a value on each $G$-orbit.\\

Consider the case where $G\subset \GL_n(k)$ and $X=k^n$. What does the space of $G$-invariant polynomials on $X$ look like? Hilbert showed that it is finitely-generated for sufficiently well-behaved $G$.\\

If $G=S_n$ for example, we get the symmetric polynomials, and this space is generated by the elementary symmetric functions, i.e. the symmetric sums of square-free monomials in each degree from 1 to $n$; we denote these $\sigma_j(x)$ for $1\leq j \leq n$.

The map $\pi:\C^n\to \C^n$ given by
$$
\pi: x\mapsto (\sigma_1(x),\dots,\sigma_n(x))
$$
has fibers exactly equal to the orbits of $S_n$.\\

Let $\zeta$ be a primitive $n$th root of unity. Consider the subgroup $\Gamma\subset \SL_2(\C)$ generated by
$$
\gamma = \begin{bmatrix}
	\zeta & 0 \\
	0 & \zeta^{-1}
\end{bmatrix}
$$
Now we can ask about $\C[x,y]^{\Gamma}$. We immediately see that $x^n,y^n,xy\in \C[x,y]^{\Gamma}$. Taking $\pi:\C^2\to \C^3$ as before, defined by
$$
(x,y)\mapsto (x^n,y^n,xy)
$$
and we again see that $\C^2/\Gamma \cong \im(\pi)$.\\

In more generality, suppose $\Gamma \subset \SL_2(\C)$ is a finite subgroup. If $f(x,y)\in \C[x,y]^{\Gamma}$, supposing $f$ is homogeneous of degree $d$, we can write $f$ as the product of $d$ lines through $0$. Then any $g\in \Gamma$ must permute these linear factors. \\

Let $G = \R^{\times}$, acting on $\R$. The orbits of the action are $(-\infty,0),\{0\},(0,\infty)$. Thus the only $G$-invariant polynomials on $\R$ are constants.

But if we let this $G$ act on $\R^2$ via $t:(x,y)\mapsto (tx,t^{-1}y)$, the orbits are halves of hyperbolas and the four pieces of the coordinate axes and the origin.\\

Let $G=\GL_n(k)$ act on $M_n(k)$. The $G$-orbits are represented by matrices in Jordan Canonical Form. Let's look at the case $n=2$. The $G$-invariant polynomials in the four matrix entries are generated by $ad-bc$, the determinant. The orbits can be made to look like hyperboloids or cones.\\

\subsection{Invariant Part} For any completely-reducible representation $V$ of $G$, there is a canonical projection from $\Inv_V: V\to V^G$ which commutes with action of $G$ (hence fixes $V^G$) and also commutes with any other $G$-intertwiner $f:V\to W$. Also $\Inv_V$ agrees with $\Inv_W$ on $W$ for any subrepresentation $W$.
\begin{proof}
	Now let's construct this $\Inv_V$. Let $A$ be an algebra over $k$ and $E$ a simple finite-dimensional $A$-module. If $M$ is another finite-dimensional $A$-module that is completely reducible, of the form 
	$$
	M = M^{(E)}\oplus M'
	$$
	where $M^{(E)}$ is the isotypic component of $E$ and $M'$ is the direct sum of other isotypic components (since $M$ is completely reducible). We want the projection $M\to M^{(E)}$. If $N=N^{(E)}\oplus N'$ is another such module, then we claim that the projections commute with any $A$-linear map $f:M\to N$. This is not so hard to check.\\
	
	To convert this proof about modules into one about representations, we take $A=kG$ as usual and say that $A$-modules are $G$-representations. We take $E$ to be the trivial representation of $G$ as a module, so that $V^{(E)}$ is the $G$-fixed points of $G$.
\end{proof}\\

Let $G\subset \GL_n(\C)$, acting on $P=\C[x_1,\dots,x_n]$. We say that $G$ is \textit{reductive} if the representation given by the action of $G$ on $P_d$ is completely reducible.\\

Examples:
\begin{itemize}
	\item Any finite group $G$ is reductive.
	\item ``Any group with a name'' is reductive ($\GL_n,\SL_n,\OO_n,\SO_n$, etc)
\end{itemize}

\medspace

A non-example is given by the group 
$$
G := \bigg\{ \begin{bmatrix}
	1 & a \\ 0 & 1 
\end{bmatrix} \; \bigg| \; a\in \C\bigg\}.
$$

\textbf{Theorem}: If $G$ is reductive then there is some $r\geq 0$ and a map $$\ph:\C^n\to \C^r$$ where each component $\ph_i$ is a homogeneous $G$-invariant polynomial of positive degree, and 
\begin{enumerate}[(1)]
	\item $\im(\ph)$ is an algebraic subset of $\C^r$.
	\item The fiber $\ph^{-1}(y)$ of any $y\in \im(\ph)$ contains a \textit{unique} closed $G$-orbit $O_y$, and moreover any $G$-orbit in $\ph^{-1}(y)$ contains $O_y$ in its closure.
	\item $\ph^{-1}(0)$ consists of the points $z\in \C^n$ for which $0\in \overline{G\cdot z}$ (i.e. the closure of the $G$-orbit of $z$ contains 0).
\end{enumerate} 
\begin{proof}
	
\end{proof}\\

Examples in some cases we looked at before:
\begin{itemize}
	\item (Non-example) If $G=\R^{>0}$ acting on $\R^2$ by $t:(x,y)\mapsto (tx,t^{-1}y)$, then we have $\ph:\R^2\to \R$ (so $r=1$) defined
	$$
	\ph:(x,y)\mapsto xy.
	$$
	The orbits look like half-hyperbolas or half-axes or the origin. The theorem tells us that $\ph^{-1}(y)$ should contain a unique closed orbit, but there are two when $y\neq 0$. So this fails because in $\R$ the orbits are not connected, whereas in $\C$ they would be.
\end{itemize}

\medspace

If $V_1,V_2\subseteq \C^n$ are two $G$-stable algebraic subsets, then there is a $G$-invariant polynomial $f$ such that $f|_{V_1}=0$ and $f|_{V_2}=1$.
\begin{proof}
	Let $I_1=I_{V_1}$ and $I_2=I_{V_2}$, since $V_1,V_2$ are algebraic. For all $p\in I_1$, $\Inv_P(p)=\Inv_{I_1}(p)$ (why?)
	
	By the Nullstellensatz, take $p_1\in I_1,\;p_2\in I_2$ such that $p_1+p_2=1$, and then take the invariant parts of each over $P$.
\end{proof}\\

\textbf{Theorem (Hilbert)}: If $G\subseteq \GL_n(\C)$ is reductive then the algebra $\C[x_1,\dots,x_n]^G$ is finitely generated.
\begin{proof}
	Let $J: = P^G_{>0}$. It suffices to show that $J$ is finitely-generated in $P^G$. We know by Hilbert basis theorem that $JP$ is a finitely-generated ideal of $P$. (proof omitted)
\end{proof}\\

So let $\ph_1,\ph_2,\dots,\ph_r$ generate $P^G$ and take a map $F:\C[x_1,\dots,x_n] \to \C[y_1,\dots,y_r]$ via
$$
F: p(x_1,\dots,x_n) \mapsto  p(\ph_1(x_1,\dots,x_n),\ph_2(x_1,\dots,x_n),\dots,\ph_r(x_1,\dots,x_n)).
$$


\subsection{Topological Groups and Integration} Almost all of our results rest on the complete reducibility of the representations we're interested in. The invariant part depends on this, for example. We initially got this from unitary representations, but in recent examples (e.g. $t:(x,y)\mapsto (tx,t^{-1}y)$ etc) we don't have an inner product yet we want to get the same results. We can get this another way: from compact topological groups. In fact, when working with these groups one can get an explicit formula for the invariant part.

Hermann Weyl showed reductivity for some non-compact groups by relating them to compact groups, using the \textit{Unitary Trick}.\\

A \textit{topological group} is a group $G$ which is also a Hausdorff topological space, with the additional property that multiplication and inversion are continuous. We also say that $G$ acts topologically on a set $X$ if the action is continuous. We will be most interested in \textit{compact} topological groups.\\

\begin{itemize}
\item Any finite group can be made into a compact topological group by giving it the discrete topology.
\item $M_n(\R)$ can be put in bijection with $\R^{n^2}$ and thus given a metric. So the group $\GL_n(\R)$ is a topological group (one has to check that multiplication and inversion are continuous). Similarly for any subgroup of $\GL_n(\R)$. 
\item Any finite-dimensional vector space under $+$ is a topological group.
\item The circle $S^1$ is a compact topological group. 
\end{itemize}

\medspace

Let $X$ be a locally compact topological space, $C(X)$ be the space of continuous functions $X\to \R$, and $C_c(X)$ the space of such functions with compact support. Let $G$ act on $X$. A Borel measure $\mu$ on $X$ is $G$-invariant if action of $G$ preserves measure of all Borel sets. It is equivalent to make the stronger statement that $G$ preserves integrals:
$$
\int_Xg^*f(x) \; \d \mu = \int_Xf(x) \; \d \mu .
$$
where $g^*f(x) = f(g^{-1}(x))$.\\

Let $L_g$ and $R_g$ be the actions of $G$ on itself by
$$
L_g:x\mapsto xg, \;\;\; R_g:x\mapsto gx.
$$
These actions commute.\\

\textbf{Theorem (Haar)}: Any locally compact topological group $G$ has a unique (up to scaling) left-invariant measure $\mu$, and also a unique right-invariant measure. They may be different in general, but if $G$ is compact then they are the same.
\begin{proof}
	The following construction is due to Von Neumann. Fix some continuous $f:G\to \R$. For any finite subset $F\subseteq G$, let the left average of $f$ at $x$ be
	$$
	A_F(f)(x) = \frac{1}{F} \sum_{g\in F} f(gx)
	$$
	and likewise for the right average. Let $\de(f)$ be 
	$$
	\de(f) := \max_{x\in G} f(x) - \min_{x\in G} f(x)
	$$
	which exists because of the local compactness of $G$. We will show that for non-constant $f$, there is a finite $F\subseteq G$ such that 
	$$
	\de(A_F(f)) < \de(f).
	$$
	And in fact $\de(A_F(f))$ is arbitrarily small as $F$ ranges over subsets of $F$. Thus $A_F(f)$ approaches a limit as $F$ gets larger, and we let this limit be the integral of $f$.
\end{proof}\\

Properties of the Haar measure:
\begin{itemize}
	\item $f\to \int f$ is linear.
	\item $f\geq 0$ for all $x$ implies $\int f\geq 0$.
	\item $\int f = 0$ iff $f=0$ everywhere (assuming $f$ is continuous).
	\item $\int 1 = 1$.
	\item $\int$ is both left- and right- $G$-invariant.
	\item $\int f(x) = \int f(x^{-1})$.
\end{itemize}
\medspace

Moreover, we can extend this measure to integrate functions $G\to V$ where $V$ is any finite-dim vector space, e.g. $\C$.

Examples:
\begin{itemize}
	\item For $\R^n$, we get the Lebesgue measure.
	\item For $S^1$, we have $\mu= \d \theta$. 
\end{itemize}


\subsection{Continuous Representations} A representation $\rho: G \to \GL_N(k)$ of a topological group $G$ is \textit{continuous} if each entry of the matrix $\rho(g)$ is continuous wrt $g$.\\

\textbf{Theorem}: Continuous irreps of $\GL_N^+$ (the matrices with positive determinant) are in bijection with descending $n$-tuples in $\Z$, which we call 		``collections."
\begin{proof}
	Let $H$ be the group of diagonal matrices in $\GL_N^+$, $B$ be the group of upper triangular matrices in $\GL_N^+$, and $U$ be the group of upper triangular matrices with 1s on the diagonal.\\
	
	Let $\rho$ be a continuous irrep of $\GL_N^+$. Then we claim that there is a nonzero eigenvector $v\in k^N$ with eigenvalue 1 for all matrices in $\rho(U)$, and eigenvalue 
	$$
	h_1^{\lambda_1}h_2^{\lambda_2}\cdots h_N^{\lambda_N}
	$$
	for $\rho(h)$, where $h$ is the diagonal matrix with entries $h_1,\dots,h_N$.\\
	
	It follows by Borel fixed point theorem applied to $\P^{N-1}$. (?)
\end{proof}\\

Let $G$ be a compact topological group acting continuously on a topological space $X$. For $f\in C(X)$, we can define 
$$
\Av(f) : X\to \C
$$
by 
$$
\Av(f)(x) = \int_G f(g^{-1}x) \; \d g.
$$
Note that $\Av(f)$ is continuous and $G$-invariant:
$$
h^*\Av(f) = \Av(h^{-1}f) = \int_G f(g^{-1}h^{-1}x) \; \d g = \int_G f((gh)^{-1}x) \d g = \Av(f)
$$
using the fact that $Gh=G$.\\

\textbf{Theorem}: Any finite-dimensional continuous representation $V$ of $G$ (a compact topological group) can be made unitary.
\begin{proof}
	Since $V$ is finite-dimensional assume wlog that $V=\C^n$ for some $n$. We have a hermitian inner product $V\times V\to \C$. Let $G$ act on $V\times V$ by 
	$$
	g:(v,w) \mapsto (g\cdot v, g\cdot w).
	$$
	We can now define a $G$-invariant inner product on $V\times V$ given by 
	$$
	\<v,w\> = \Av((v,w)) = \int_G (g^{-1}\cdot v,g^{-1}\cdot w) \; \d g.
	$$
	To show that it is an inner product we must check that it is linear, which follows because $(\bullet,\bullet)$ is linear and the integral is linear and $G$ acts linearly on $V$. We also check that it is positive definite, which follows from $(\bullet,\bullet)$ being positive definite.
\end{proof}\\

As a corollary, we see that any finite-dimensional continuous representation of a compact topological group is completely reducible. In particular, this is true of finite-dimensional representations of finite topological groups, which are always compact under the discrete topology, and all of their representations are continuous. This is essentially Maschke's Theorem.\\

Fix a partition $\lambda$ of $n$. Let $O_{\lambda}$ be the conjugacy class of all nilpotent matrices in $M_n(\C)$ whose Jordan blocks are the partition $\lambda$. The closure $\overline{O_\lambda}$ is an algebraic subset of $M_n(\C)$.

Let $i$ be the inclusion of diagonal matrices into $M_n(\C)$ and $i^*:\C[M_n(\C)]\to \C[x_1,\dots,x_n]$ be the corresponding restriction map of algebras. Take the ideal
$$
J_\lambda = i^*(I_{\overline{O_{\lambda}}})
$$
and note that $V(J_{\lambda})=0$ because $O_{\lambda}$ is entirely nilpotent. Thus by the nullstellensatz, we have
$$
\sqrt{J_{\lambda}} = (x_1,\dots,x_n)
$$
so $A := \C[x_1,\dots,x_n]/J_{\lambda}$ is a finite-dimensional $\C$-algebra, since for each $j$, $x_j^k\in J_{\lambda}$ for some $k$. Thus we can apply Maschke's Theorem to see that this algebra is completely reducible, so it can be written as
$$
A = \bigoplus something
$$
a graded algebra. Note also that $S_n$ acts on $A$ by graded algebra automorphisms. Let $d_{\lambda}$ be the maximum $d$ such that $A_d\neq 0$ (the grade of $A$). Then the amazing thing\footnote{Ginzburg: ``there is no explanation.''} is
$$
(P/J_{\lambda})_{d_{\lambda}} \cong V(\lambda).
$$
This is pretty new stuff. It's a special case of the Hikita conjecture.\\

\subsection{Calculus on Banach Algebras} A \textit{Banach Algebra} is an algebra over $\R$ or $\C$ equipped with a complete multiplicative norm. So essentially it is a Banach space that is also an algebra. A key example is any matrix algebra.

Let $D_r(A)$ be the open disk of radius $r$ in $A$, i.e.
$$
D_r(A) := \{x\in A : |x| < r\}.
$$
If $a_0,a_1,\dots$ is any infinite sequence in $A$, and $f(x)$ is the power series
$$
f(x) = \sum_{k=0}^{\infty} a_kx^k
$$
then $f$ converges within the radius
$$
r(f) := \limsup_{k\to\infty} |a_n|^{-1/n}
$$
which is known as the Cauchy-Hadamar radius of convergence. It is important to note that this $r(f)$ is the same for equivalent norms, as any constant factor goes to 1 in the long tail of $|a_n|^{-1/n}$.\\

\textbf{Proposition}: For all $r<r(f)$, $f(x)$ converges within $D_r(A)$ to a continuous function. Moreover, the map $D_r(\R)\to A$ given by $t\mapsto f(t \cdot 1_A)$ is smooth.
\begin{proof}
	Mostly identical to the normal calculus version.
\end{proof} \\

Examples:
\begin{itemize}
	\item For $f(x)=e^x=\sum_{k\geq 0} x^k/k!$, we have $\infty = \limsup_k |1/k!|^{-1/k}$ so $e^x$ is defined everywhere.
	\item For $f(x)=\log(1+x)$, $\sum_{i=1}^{\infty} (-1)^{i-1} x^i/i$, then $r(f)=1$.
	\item For $f(x)=1+x+x^2+\dots = (1-x)^{-1}$, $r(f)=1$. This gives that inversion is defined and continuous on the open disk of radius $1-\ep$ around $1_A$.
	\item If we conjugate $f(x)$ by $g$, yielding $f(gxg^{-1})$, then the radius of convergence becomes 
	$$
	\min\bigg\{r(f),\frac{r(f)}{|g|\cdot |g^{-1}|}\bigg\}
	$$
	note that we do not necessarily have $|g^{-1}|= |g|^{-1}$.
\end{itemize}

\medspace

Properties of the exponential map:
\begin{itemize}
	\item If $a,b$ commute (they may not in the matrix case!) then $e^{a+b}=e^{a}\cdot e^b$. The proof is the same as it would be in a field.
	\item $e^a$ is always invertible, and its inverse is $e^{-a}$ (note that $a,-a$ commute).
	\item $t\mapsto e^{ta}$ is a continuous group homomorphism from $\R$ to $A^{\times}$.
	\item For sufficiently small $r>0$, $\exp(D_r(A))$ is an open neighborhood of $1_A$, and there are mutually inverse homeomorphisms $\exp$ and $\log$ between $D_r(A)$ and $\exp(D_r(A))$.   
\end{itemize}

\medspace

\subsection{Lie Groups} Let $V,W$ be normed vector spaces and let $U$ be an open subset of $V$. For maps $f:U\to W$, we say that $f$ is \textit{differentiable} at $x\in U$ if there is a linear map $d_x f:V\to W$ such that 
$$
f(x+v) = f(x) + (d_xf)(v) + o(v)
$$ 
within a sufficiently small neighborhood of $x$.\\

For example, taking the function $\exp:M_n(\R)\to M_n(\R)$, we have $d_0\exp = \id$.\\

\textbf{Lemma}: Let $M_n=V_1\oplus V_2$ be a decomposition of $M_n(\R)$. Define the map $\Phi:M_n\to M_n$ by 
$$
\Phi: (v_1,v_2) \mapsto e^{v_1}e^{v_2}. 
$$
Then there exist sufficiently small disks $D_1\subset V_1$ and $D_2\subset V_2$ centered at 0 such that $\Phi(D_1\times D_2)$ is an open neighborhood of $\id\in \GL_n(\R)$, and $\Phi$ is a diffeomorphism on $D_1\times D_2$.
\begin{proof}
	It suffices to show that $d_0\Phi$ exists and is invertible. In fact, it is equal to the identity. By linearity,
	$$
	d_0\Phi(v_1,v_2) = d_0\Phi(v_1) + d_0\Phi(v_2) = \id_{V_1} + \id_{V_2} = \id.
	$$
\end{proof}\\

\textbf{Lemma}: For matrices $x,y\in M_n$, we have the approximation
$$
e^{x/N}e^{y/N} = e^{(x+y+\a_N)/N}
$$
where $\a_N\in M_n$ such that $\lim_N \a_N = 0$.\\

The significance of this is that while $e^{x+y}\neq e^xe^y$ in general, they are close for small-norm matrices.\\

\textbf{Limit Lemma}: Let $G\subset \GL_n$ be a closed subgroup. If $x_i\in M_n$ is a sequence such that 
\begin{enumerate}[(1)]
	\item $e^{x_i}\in G$
	\item $|x_i|\to 0$ but $x_i\neq 0$
	\item $x_i/|x_i|\to x$ where $|x|=1$
\end{enumerate} 
then $e^{tx}\in G$ for all $t\in \R$.
\begin{proof}
	Fix $t$. Choose integers $m_i$ so that $m_i|x_i|$ is as close as possible to $t$. Since $|x_i|\to 0$, $m_i|x_i|\to t$, and $m_ix_i\to tx$. But now since $m_i$ is an integer we can say
	$$
	e^{m_ix_i} = (e^{x_i})^{m_i} \in G 
	$$
	and $e^{m_ix_i}\to e^{tx}$ so because $G$ is closed, $e^{tx}\in G$.
\end{proof}\\

A \textit{Lie Group} is a closed subgroup $G\subseteq \GL_n(\R)$. Corresponding to a Lie group we have a Lie algebra of $G$,
$$
\Lie(G) = \{x\in M_n : e^{tx}\in G \; \forall t\in \R\}
$$
To show that this is actually an algebra requires some work. Note that $\Lie(G)$ is automatically closed under scaling and multiplication. To show that it is also closed under addition, we use the lemmas: if $a,b\in \Lie(G)$, then 
$$
e^{a/N} e^{b/N} \in G \implies e^{(a+b+\a_n)/N} \in G
$$
where $\a_N\to 0$. Take $x_N = (a+b+\a_N)/N$. Now by the limit lemma, $x_N$ satisfies all the conditions and $a+b=x$, so indeed $e^{x}=e^{a+b} \in G$ as desired.
\\

Examples:
\begin{itemize}
	\item For $G=\GL_n(\R)$, $\Lie(G)=G$.
	\item For $G=\SL_n(\R)$, 
	$$
	\Lie(G) = \{x\in M_n : \det(e^{tx}) =1 \forall t\} = \{x\in M_n : e^{t\tr(x)} = 1\} = \{x\in M_n : \tr(x)=0\}.
	$$
	\item 
\end{itemize}

\medspace

\textbf{Cartan's Theorem}: Let $G\subseteq \GL_n(\R)$ be a Lie group with unit $e$. Then there is an open neighborhood of $\mathbf{0}_n\in M_n(\R)$ such that
$$
\exp(U\cap \Lie(G)) = \exp(U)\cap G.
$$
\begin{proof}
	
\end{proof}



\end{document}